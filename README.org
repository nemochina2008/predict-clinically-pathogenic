** 10 Apr 2017

- [[file:train_model.R]] now excludes some features for multivariate model
  training (but still uses them for univariate models).
- [[file:predict_scores.R]] creates NAkeep and NAguess scores for each
  univariate model -- NAkeep scores may have missing values (NA) but
  missing values in NAguess scores are replaced with the most frequent
  class in the missing values in the training set.
- [[file:plot_ROC.R]] interprets scores with missing values as errors,
  computes more test error/accuracy metrics, and saves them to
  test.txt_predictions.RData.csv

** 15 Feb 2017
Edit [[file:BP.other.models.R]] to support using some features for testing
single variable models but not for training multi-variate models.
** 14 Nov 2016

R scripts revised so that we can compute ROC curves for input features
that have missing data.

** 9 Nov 2016

New R scripts that can be run with command line arguments, documented
in [[file:example.sh]]
** 4 Nov 2016

[[file:predictions.no.folds.R]] trains and tests models on whole data sets
(without doing cross-validation). Predicted scores for each model and
data set are written to the predicted-scores directory.

** 1 Nov 2016

Bugfix: xgboost test error is computed using threshold of 0.5 (before,
there was no threshold specified).

More portable ggplot2 code.

** 12 Oct 2016

[[http://cbio.mines-paristech.fr/~thocking/figure-BP-other-models/][Interactive figure with ROC curves]].

** 5 Oct 2016

[[file:BP.other.models.R]] performs a fold-fold cross-validation
experiment with four different data sets, and the new xgboost model.

[[file:figure-BP-other-models.R]] makes a figure that summarizes the
prediction accuracy of the different models in the four-fold
cross-validation experiment. It is clear that xgboost is as good as,
and sometimes better than, the single variable models, for all train
and test sets.

[[file:figure-BP-other-models-four.png]]

** 19 Aug 2016
[[
file:figure-BP-other-models.R]] makes this interactive figure [[http://bl.ocks.org/tdhock/raw/5dbfe365b3329a6155f027e141cbf803/]] and the following static figures 

[[file:figure-BP-other-models-accuracy.png]]

[[file:figure-BP-other-models-auc.png]]

These data indicate that the models trained on one data set yield
highly accurate predictions on the other data. The multi-feature model
is also just about as good as the best single-feature model
(VEST3_score). The reason why these results are different from the
previous cross-validation experiment is the treatment of NA values by
the baseline methods. Before those baseline models were just ignoring
the NA values. Now those baseline models will predict whatever label
is more frequent for the NA values in the training set. For example
this makes a big difference for the VEST3_score, because almost all
the observations with NA for VEST3_score are Pathogenic.

** 18 Aug 2016

- new data from Najmeh, with 16 numeric features.
- [[file:some.models.R]] computes single-variable and 16-variable models
  (not all pairs of two-variable models, which takes a long time with
  so many features).
- [[file:figure-some-test-error.R]] plots the test error of each model. It
  is clear that the tree and forest models are more accurate than any
  single variable model, and that they are about as accurate as
  several two-variable models, such as ctree or cforest with
  VEST3_score and CADD_phred.

[[file:figure-some-test-error.png]]

** 18 Feb 2016

Computed the test error for the best univariate thresholding
classifier using each feature. The best univariate thresholding models
are Polyphen2 and CADD, but the multivariate ctree and cforest models
definitely seem to be more accurate. 

[[file:figure-test-error.png]]

I also fit ctree and cforest models for all pairs of scores. The pair
of scores that comes closest to the 7-score model is CADD and SIFT. I
used them to create the following visualization of the predicted
probabilities of the tree and forest models (when trained in this
two-dimensional feature space).

[[file:figure-two-features.png]]

While talking over tea Simon recommended reading about related methods
- http://www.columbia.edu/~ii2135/Eigen_11_24.pdf
- http://www.pnas.org/content/111/4/1253.full.pdf

** 17 Feb 2016

First train data set from Najmeh, 1200 observations (genomic
variants/positions) x 7 scores.

Computed test error via 5-fold CV of tree and forest models, which are
definitely more accurate than a trivial majority classifier.
